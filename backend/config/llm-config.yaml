# LLM Router Configuration
# This file contains the configuration for the LLM router

# Default settings for all providers
defaults:
  # Default provider to use if not specified in the request
  default_provider: "openai"
  
  # Default model to use if not specified in the request
  default_model: "gpt-4-turbo"
  
  # Load balancing strategy (round_robin or cost_priority_round_robin)
  load_balancing: "cost_priority_round_robin"

# Provider configurations
providers:
  # OpenAI Configuration
  openai:
    name: "openai"
    type: "openai"
    # API key can be set here or through environment variable LLM_OPENAI_API_KEY
    api_key: ${LLM_OPENAI_API_KEY}
    base_url: "https://api.openai.com/v1"
    priority: 1  # Higher number = higher priority
    enabled: true
    
    # Model configurations
    models:
      - name: "gpt-4-turbo"
        cost_per_1k_input_tokens: 0.01
        cost_per_1k_output_tokens: 0.03
        max_tokens: 128000
        rate_limit:
          tokens_per_minute: 40000
          requests_per_minute: 200
    
    # Circuit breaker configuration
    circuit_breaker:
      failure_threshold: 5  # Number of failures before opening the circuit
      success_threshold: 3   # Number of successes required to close the circuit
      timeout: 30000         # Time in ms to wait before testing the circuit again
    
    # Retry configuration
    retry:
      max_attempts: 3
      initial_delay: 1000    # Initial delay in ms
      max_delay: 30000       # Maximum delay in ms
      factor: 2              # Exponential backoff factor

  # Anthropic Configuration
  anthropic:
    name: "anthropic"
    type: "anthropic"
    api_key: ${LLM_ANTHROPIC_API_KEY}
    base_url: "https://api.anthropic.com"
    priority: 2
    enabled: true
    
    models:
      - name: "claude-3-opus-20240229"
        cost_per_1k_input_tokens: 0.015
        cost_per_1k_output_tokens: 0.075
        max_tokens: 200000
        rate_limit:
          tokens_per_minute: 30000
          requests_per_minute: 100
    
    circuit_breaker:
      failure_threshold: 5
      success_threshold: 3
      timeout: 30000
    
    retry:
      max_attempts: 3
      initial_delay: 1000
      max_delay: 30000
      factor: 2

  # Google Gemini Configuration
  google:
    name: "google"
    type: "google"
    api_key: ${LLM_GOOGLE_API_KEY}
    base_url: "https://generativelanguage.googleapis.com"
    priority: 3
    enabled: true
    
    models:
      - name: "gemini-pro"
        cost_per_1k_input_tokens: 0.00025
        cost_per_1k_output_tokens: 0.0005
        max_tokens: 30720
        rate_limit:
          tokens_per_minute: 60000
          requests_per_minute: 300
    
    circuit_breaker:
      failure_threshold: 5
      success_threshold: 3
      timeout: 30000
    
    retry:
      max_attempts: 3
      initial_delay: 1000
      max_delay: 30000
      factor: 2

  # Ollama Configuration (local)
  ollama:
    name: "ollama"
    type: "ollama"
    base_url: ${OLLAMA_BASE_URL:-http://localhost:11434}
    priority: 4  # Lowest priority as it's a fallback
    enabled: true
    
    models:
      - name: "llama3"
        cost_per_1k_input_tokens: 0.0
        cost_per_1k_output_tokens: 0.0
        max_tokens: 8192
        rate_limit:
          tokens_per_minute: 100000
          requests_per_minute: 1000
    
    circuit_breaker:
      failure_threshold: 3
      success_threshold: 1
      timeout: 10000
    
    retry:
      max_attempts: 2
      initial_delay: 500
      max_delay: 10000
      factor: 2

# Environment variable overrides
# All settings can be overridden using environment variables with the following format:
# LLM_<PROVIDER>_<SETTING>=value
# Example: LLM_OPENAI_API_KEY=sk-...
#          LLM_ANTHROPIC_ENABLED=false
